{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><b>SUPER RESOLUTION</b></h1></center>\n",
    "<center><h4>Using Generative adversarial network</h4></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Requisites:\n",
    "<ul style=\"list-style-type:circle;\">\n",
    "    <li>Python</li>\n",
    "    <li>Anaconda </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification:\n",
    "<pre>\n",
    " * GPU:- Nvidia Geforce RTX 2080 Ti\n",
    " * Tensorflow 1.14.0 \n",
    " * Keras 2.2.4\n",
    " * Scipy 1.2.0\n",
    " * Data Dimensions:  \n",
    "         High Resolution: 512*512\n",
    "         Low Resolution : 256*256\n",
    " * Datset: 16000 images of both High and Low resolution images.\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Required:\n",
    "<ul style=\"list-style-type:square;\">\n",
    "    <li><b>Keras:</b> Keras is an open-source neural-network library written in Python. It is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit.\n",
    "    <li><b>Tensorflow:</b> TensorFlow is a free and open-source software library for dataflow and differentiable programming across a range of tasks.\n",
    "    <li><b>Skimage:</b> Scikit-image is a collection of algorithms for image processing\n",
    "    <li><b>Numpy:</b> NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "    <li><b>Scipy:</b> SciPy is a free and open-source Python library used for scientific computing and technical computing.\n",
    "    <li><b>Matplotlib:</b> Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy.\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <b><h2>Importing Libraries</h2></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5356be857c6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import keras\n",
    "import sys\n",
    "import cv2\n",
    "import image_slicer\n",
    "\n",
    "import tensorflow as tf\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pltimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "\n",
    "from numpy import array\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, add\n",
    "from keras.layers import Lambda, Input\n",
    "from keras.layers.core import Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.models import load_model, Model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "from skimage.transform import rescale, resize\n",
    "from scipy.misc import imresize\n",
    "from skimage import img_as_ubyte\n",
    "from skimage import data, io, filters\n",
    "from skimage import measure\n",
    "\n",
    "from numpy import array\n",
    "from numpy.random import randint\n",
    "from numpy import expand_dims\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b><h2>Data Loading and Data Preprocessing: </h2></b>\n",
    "<b>Data Loading is the process that involves taking the data and loading it where the users can access it.</b>\n",
    "<br>Function load_path is used for getting all the files present in the given directory. It returns the full path to file from the current directory.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_path(path):\n",
    "    directories = []\n",
    "    if os.path.isdir(path):\n",
    "        directories.append(path)\n",
    "    for elem in os.listdir(path):\n",
    "        if os.path.isdir(os.path.join(path,elem)):\n",
    "            directories = directories + load_path(os.path.join(path,elem))\n",
    "            directories.append(os.path.join(path,elem))\n",
    "    return directories\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data Loading from the directory.</b>\n",
    "<br>This function is used to load the files from a specific directory. This uses load_path function to load the path.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_dirs(directory, ext, no_of_images):\n",
    "    dirs = load_path(directory)\n",
    "    files = []\n",
    "    file_names = []\n",
    "    count = 0\n",
    "    for d in dirs:\n",
    "        for f in os.listdir(d): \n",
    "            if f.endswith(ext):\n",
    "                image = data.imread(os.path.join(d,f))\n",
    "                if len(image.shape) > 2:\n",
    "                    files.append(image)\n",
    "                    file_names.append(os.path.join(d,f))\n",
    "                count = count + 1\n",
    "                if count >= no_of_images :\n",
    "                        return files\n",
    "    return files     \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Loading data and resizing the files.</b>\n",
    "<br>This function is used for loading the images from the directory and resizing the images.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_dirs_resize(directory, ext, size, no_of_images):\n",
    "    dirs = load_path(directory)\n",
    "    files = []\n",
    "    file_names = []\n",
    "    count = 0\n",
    "    for d in dirs:\n",
    "        for f in os.listdir(d): \n",
    "            if f.endswith(ext):\n",
    "                rimg = resize(data.imread(os.path.join(d,f)), size)\n",
    "                pltimg.imsave('./mod/' + 'image_%d.jpg' % count, rimg)\n",
    "                files.append(rimg)\n",
    "                file_names.append(os.path.join(d,f))\n",
    "                count = count + 1\n",
    "                if count >= no_of_images :\n",
    "                    return files\n",
    "    return files     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Loading higher resolution images for the model.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data_for_model(directory, ext, number_of_images = 100):\n",
    "    files = load_data_from_dirs(directory, ext, number_of_images)\n",
    "    if len(files) < number_of_images:\n",
    "        print(\"Number of image files are less then you specified\")\n",
    "        print(\"Please reduce number of images to %d\" % len(files))\n",
    "        sys.exit()\n",
    "    x_test_hr = preprocess(files, 'hr')\n",
    "\n",
    "    return x_test_hr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Loading lower resolution images for the model.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data_for_model1(directory, ext, number_of_images = 100): #for lr\n",
    "    files = load_data_from_dirs(directory, ext, number_of_images)\n",
    "    if len(files) < number_of_images:\n",
    "        print(\"Number of image files are less then you specified\")\n",
    "        print(\"Please reduce number of images to %d\" % len(files))\n",
    "        sys.exit()\n",
    "    x_test_lr = preprocess1(files, 'lr')\n",
    "    \n",
    "    return x_test_lr\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to load the test data i.e lower resolution images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(directory, ext, number_of_images = 100):\n",
    "    files = load_data_from_dirs(directory, ext, number_of_images)\n",
    "    if len(files) < number_of_images:\n",
    "        print(\"Number of image files are less then you specified\")\n",
    "        print(\"Please reduce number of images to %d\" % len(files))\n",
    "        sys.exit()    \n",
    "    x_test_lr = preprocess1(files, 'lr')\n",
    "    \n",
    "    return x_test_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "<b>Data processing is, generally, \"the collection and manipulation of items of data to produce meaningful information.\"</b>\n",
    "<pre>\n",
    "Test size is 0.2 out of 1.\n",
    "Function <b>preprocess</b> and <b>preprocess1</b> are used to normalizing HR and LR images to -1 to 1 range. The reason we do both of those things is because in the process of training our network, we're going to be multiplying (weights) and adding to (biases) these initial inputs in order to cause activations that we then backpropogate with the gradients to train the model. We'd like in this process for each feature to have a similar range so that our gradients don't go out of \n",
    "control.\n",
    "(images_hr.astype(np.float32) - 127.5)/127.5 = convert values from [0,255] to  [-1, 1] i.e if the pixel value is 0, then (0-1227.5)/127.5 gives -1. Similarly it converts all the other pixel to the specified range.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "def preprocess(files,ip=None):\n",
    "  brk = int((1-test_size)*len(files))\n",
    "  x_train = files[:brk]\n",
    "  x_test = files[brk:]\n",
    "  \n",
    "  def hr_images_norm(images):\n",
    "    images_hr = array(images)\n",
    "    return (images_hr.astype(np.float32) - 127.5)/127.5\n",
    "  \n",
    "  x_train_hr = hr_images_norm(x_train)\n",
    "  x_test_hr = hr_images_norm(x_test)\n",
    "    \n",
    "  if(ip == 'hr'):\n",
    "    return x_test_hr\n",
    "  \n",
    "  return x_train_hr,x_test_hr\n",
    "\n",
    "def preprocess1(files1,ip=None):\n",
    "  brk = int((1-test_size)*len(files1))\n",
    "  x_train = files1[:brk]\n",
    "  x_test = files1[brk:]\n",
    "  \n",
    "  def lr_images_norm(images_real):\n",
    "    images_lr = array(images_real)\n",
    "    return (images_lr.astype(np.float32) - 127.5)/127.5\n",
    "  \n",
    "  x_train_lr = lr_images_norm(x_train)\n",
    "  x_test_lr = lr_images_norm(x_test)\n",
    "  \n",
    "  if(ip == 'lr'):\n",
    "    return x_test_lr\n",
    "  \n",
    "  return x_train_lr,x_test_lr\n",
    "\n",
    "def denormalize(input_data):\n",
    "    print('Denormalizing....')\n",
    "    input_data = ((input_data + 1) * 127.5).astype(np.uint8)\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparattion.\n",
    "Use this code only for preparing the dataset. Otherwise you can directly use the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './testhr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6bec27dbc073>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minput_dir_HR\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./testhr'\u001b[0m \u001b[1;31m#Please mention the path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0minput_dir_LR\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;34m'./test_LR/'\u001b[0m \u001b[1;31m#Please mention the path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mimage_list_HR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dir_HR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mimage_list_LR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dir_LR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage_slicer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: './testhr'"
     ]
    }
   ],
   "source": [
    "#this code snippet is used to divide the images into 4 exact parts.\n",
    "input_dir_HR='./testhr' #Please mention the path\n",
    "input_dir_LR  = './test_LR/' #Please mention the path\n",
    "image_list_HR = os.listdir(input_dir_HR)\n",
    "image_list_LR = os.listdir(input_dir_LR)\n",
    "import image_slicer\n",
    "for x in range(1379):\n",
    "    tiles=image_slicer.slice(image_list_HR[x], 4, save=False)\n",
    "    image_slicer.save_tiles(tiles, directory='./kk', prefix='slice_'+ str(x) , format='jpg')\n",
    "    \n",
    "    \n",
    "#this code snippet is used for scaling the high resolution image to twice that of the higher resolution image.   \n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "input_dir_HR='./TestLR_New'\n",
    "image_list_HR = os.listdir(input_dir_HR)\n",
    "image_list_HR.sort()\n",
    "for x in range(1380):\n",
    "  image=plt.imread(image_list_HR[x])\n",
    "  image_rescaled = resize(image, (int(image.shape[0] * 2), int(image.shape[1] * 2)), anti_aliasing=False)\n",
    "  os.chdir(\"/gdrive/My Drive/Colab Notebooks/SRGAN-tensorflow/data/LR_img\")\n",
    "  plt.imsave(str(image_list_HR[x]) ,image_rescaled)\n",
    "  os.chdir(\"/gdrive/My Drive/Colab Notebooks/SRGAN-tensorflow/data/HR_img\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images(output_dir, epoch, generator, x_test_hr, x_test_lr , dim=(1, 3), figsize=(15, 5)):\n",
    "    examples = x_test_hr.shape[0]\n",
    "    value = randint(0, examples)\n",
    "    image_batch_hr = denormalize(x_test_hr)\n",
    "    image_batch_lr = x_test_lr\n",
    "    gen_img = generator.predict(image_batch_lr)\n",
    "    generated_image = denormalize(gen_img)\n",
    "    image_batch_lr = denormalize(image_batch_lr)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    plt.subplot(dim[0], dim[1], 1)\n",
    "    plt.imshow(image_batch_lr[value], interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "        \n",
    "    plt.subplot(dim[0], dim[1], 2)\n",
    "    plt.imshow(generated_image[value], interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(dim[0], dim[1], 3)\n",
    "    plt.imshow(image_batch_hr[value], interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir + 'generated_image_%d.png' % epoch)\n",
    "    \n",
    "# Plots and save generated images(in form LR, SR, HR) from model to test the model \n",
    "# Save output for all images given for testing  \n",
    "def plot_test_generated_images_for_model(output_dir, generator, x_test_hr, x_test_lr , dim=(1, 3), figsize=(15, 5)):\n",
    "    examples = x_test_hr.shape[0]\n",
    "    image_batch_lr = x_test_lr\n",
    "    gen_img = generator.predict(image_batch_lr)\n",
    "    generated_image = denormalize(gen_img)\n",
    "    image_batch_lr = denormalize(image_batch_lr)\n",
    "    image_batch_hr = denormalize(x_test_hr)\n",
    "    for index in range(examples):\n",
    "        print(\"Plotting....\")\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.subplot(dim[0], dim[1], 1)\n",
    "        plt.imshow(image_batch_lr[index], interpolation='nearest')\n",
    "        d1 = image_batch_lr[index].shape\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Low-Resolution Image : %dx%dx%d\"%(d1[0],d1[1],d1[2]))\n",
    "        plt.subplot(dim[0], dim[1], 2)\n",
    "        plt.imshow(generated_image[index], interpolation='nearest')\n",
    "        d2 = generated_image[index].shape\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Super-Resolution Image: %dx%dx%d\"%(d2[0],d2[1],d2[2]))\n",
    "        plt.subplot(dim[0], dim[1], 3)\n",
    "        plt.imshow(image_batch_hr[index], interpolation='nearest')\n",
    "        d3 = generated_image[index].shape\n",
    "        plt.axis('off')\n",
    "        plt.title(\"High-Resolution Image: %dx%dx%d\"%(d3[0],d3[1],d3[2]))\n",
    "        plt.tight_layout()\n",
    "        print(\"Saving to output_dir....\")\n",
    "        plt.savefig(output_dir + 'test_generated_image_%d.jpg' % index)\n",
    "\n",
    "# Takes LR images and save respective HR images\n",
    "def plot_test_generated_images(output_dir, generator, x_test_lr):\n",
    "    examples = x_test_lr.shape[0]\n",
    "    image_batch_lr = x_test_lr\n",
    "    gen_img = generator.predict(image_batch_lr)\n",
    "    generated_image = denormalize(gen_img)\n",
    "    for index in range(examples):\n",
    "        pltimg.imsave(output_dir + 'high_res_result_image_%d.jpg' % index, generated_image[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <center><h2>Network Architecture</h2> </center>\n",
    "<img src=\"SRGAN.jpg\" alt=\"SRGAN Diagram.  PLEASE GIVE PROPER PATH TO LOAD IMAGES.\" width=\"700\" height=\"700\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Residual Block </center>\n",
    "<img src=\"resnet.JPG\" alt=\"Resnet Block Diagram.  PLEASE GIVE PROPER PATH TO LOAD IMAGES.\" align=\"center\">\n",
    "Shape of our HR image is 512*512. So that we can compare it with the super resolution image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (512,512,3)\n",
    "def res_block_gen(model, kernal_size, filters, strides):    \n",
    "    gen = model\n",
    "    model = Conv2D(filters = filters, kernel_size = kernal_size, strides = strides, padding = \"same\")(model)\n",
    "    model = BatchNormalization(momentum = 0.5)(model)\n",
    "    # Using Parametric ReLU\n",
    "    model = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(model)\n",
    "    model = Conv2D(filters = filters, kernel_size = kernal_size, strides = strides, padding = \"same\")(model)\n",
    "    model = BatchNormalization(momentum = 0.5)(model)\n",
    "    model = add([gen, model])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling Block\n",
    "<img src=\"upsampling.JPG\" alt=\"Upsampling Block Diagram. PLEASE GIVE PROPER PATH TO LOAD IMAGES.\"  width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_sampling_block(model, kernal_size, filters, strides):\n",
    "    model = Conv2D(filters = filters, kernel_size = kernal_size, strides = strides, padding = \"same\")(model)\n",
    "    model = UpSampling2D(size = 2)(model)\n",
    "    model = LeakyReLU(alpha = 0.2)(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Block\n",
    "<img src=\"discriminator1.JPG\" alt=\"Discriminator Diagram.  PLEASE GIVE PROPER PATH TO LOAD IMAGES.\"  width=\"300\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_block(model, filters, kernel_size, strides):\n",
    "    model = Conv2D(filters = filters, kernel_size = kernel_size, strides = strides, padding = \"same\")(model)\n",
    "    model = BatchNormalization(momentum = 0.5)(model)\n",
    "    model = LeakyReLU(alpha = 0.2)(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Network Architecture of Generator</center>\n",
    "<img src=\"gen.JPG\" alt=\"Generator Diagram. PLEASE GIVE PROPER PATH TO LOAD IMAGES.\"  width=\"800\" height=\"800\">\n",
    "<pre>\n",
    "The generator aims at reproducing sharp images. The network is based on ResNet blocks. The core is 16 ResNet blocks applied to an upsampling of the original image. This ResNet layer is basically a convolutional layer, with input and output added to form the final output.\n",
    "\n",
    "The final layer outputs a 512x512x3 tensor squashed between values of -1 and 1 through the Hyperbolic Tangent (tanh) function. Finally, we scale the input data to the interval of -1 to 1 to follow the choice of using the tanh function\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "\n",
    "    def __init__(self, noise_shape):\n",
    "        \n",
    "        self.noise_shape = noise_shape\n",
    "\n",
    "    def generator(self):\n",
    "                \n",
    "        gen_input = Input(shape = self.noise_shape)\n",
    "        model = Conv2D(filters = 64, kernel_size = 9, strides = 1, padding = \"same\")(gen_input)\n",
    "        model = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(model)\n",
    "        gen_model = model\n",
    "        \n",
    "    # Using 16 Residual Blocks\n",
    "        for index in range(16):\n",
    "            model = res_block_gen(model, 3, 64, 1)\n",
    "    \n",
    "        model = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = \"same\")(model)\n",
    "        model = BatchNormalization(momentum = 0.5)(model)\n",
    "        model = add([gen_model, model])\n",
    "    \n",
    "    # Using 1 UpSampling Blocks\n",
    "        for index in range(1):\n",
    "            model = up_sampling_block(model, 3, 64, 1)\n",
    "    \n",
    "        model = Conv2D(filters = 3, kernel_size = 9, strides = 1, padding = \"same\")(model)\n",
    "        model = Activation('tanh')(model)\n",
    "   \n",
    "        generator_model = Model(inputs = gen_input, outputs = model)\n",
    "        #print(generator_model.summary())\n",
    "        return generator_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Network Architecture of Discriminator</center>\n",
    "<img src=\"discriminator.JPG\" alt=\"Discriminator Diagram.  PLEASE GIVE PROPER PATH TO LOAD IMAGES.\"  width=\"800\" height=\"800\">\n",
    "<pre>\n",
    "The discriminator is also a 7 layer CNN with BN (except its input layer) and leaky RELU activations.Leaky ReLUs are very popular because they help the gradients flow easier through the architecture.\n",
    "\n",
    "A regular ReLU function works by truncating negative values to 0. This has the effect of blocking the gradients to flow through the network. Instead of the function being zero, leaky RELUs allow a small negative value to pass through. That is, the function computes the greatest value between the features and a small factor.\n",
    "\n",
    "The discriminator starts by receiving a 512x512x3 image tensor. The discriminator performs a series of strided 1 and 2  convolutions. \n",
    "\n",
    "Finally, the discriminator needs to output probabilities. For that, we use the Logistic Sigmoid activation function on the final logits.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(object):\n",
    "    #image_shape = (512,512,3)\n",
    "    def __init__(self, image_shape):\n",
    "        \n",
    "        self.image_shape = image_shape\n",
    "    \n",
    "    def discriminator(self):\n",
    "        \n",
    "        dis_input = Input(shape = self.image_shape)\n",
    "        \n",
    "        model = Conv2D(filters = 32, kernel_size = 3, strides = 1, padding = \"same\")(dis_input)\n",
    "        model = LeakyReLU(alpha = 0.2)(model)\n",
    "        \n",
    "        model = discriminator_block(model, 32, 3, 2)\n",
    "        model = discriminator_block(model, 64, 3, 1)\n",
    "        model = discriminator_block(model, 64, 3, 2)\n",
    "        model = discriminator_block(model, 128, 3, 1)\n",
    "        model = discriminator_block(model, 128, 3, 2)\n",
    "        model = discriminator_block(model, 256, 3, 1)\n",
    "        model = discriminator_block(model, 256, 3, 2)\n",
    "        \n",
    "        model = Flatten()(model)\n",
    "        model = Dense(1024)(model)\n",
    "        model = LeakyReLU(alpha = 0.2)(model)\n",
    "       \n",
    "        model = Dense(1)(model)\n",
    "        model = Activation('sigmoid')(model) \n",
    "        \n",
    "        discriminator_model = Model(inputs = dis_input, outputs = model)\n",
    "        \n",
    "        return discriminator_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy Loss:\n",
    "\n",
    "<pre>\n",
    "Formula Derivation:\n",
    "Loss function for Binary cross Entropy is given by :\n",
    "<img src=\"binary_loss.png\" alt=\"Loss formula Diagram.  PLEASE GIVE PROPER PATH TO LOAD IMAGES.\"  width=\"800\" height=\"800\">\n",
    "where N is the number of training example.\n",
    "L(yhat,y)=y*log(yhat) + (1-y)*log(1-yhat)  -------------(1)\n",
    "Where yhat is the Reconstructed Image and y is the Original image\n",
    "The label for the data coming from real distribution is Y=1 and Yhat =D(x) where x is the probability distribution of random variable X. D(x) is the discriminator output for real data distribution. From (1)\n",
    "L(D(x),1) = log(D(x))        -------------(2)\n",
    "And for data coming from generator the label is y=0 and yhat=D(G(z)) where z is the random probability distribution of random variable Z. D(G(z)) is the output of discriminator.  From (1)\n",
    "L(D(G(z),0) = log(1-D(G(z))  ------------(3)\n",
    "The objective of the Discriminator is to correctly classify the real and fake data. Hence eqn(1)\tand eqn(2) has to be maximized.\n",
    "<img src=\"1.JPG\" alt=\"Loss formula Diagram.  PLEASE GIVE PROPER PATH TO LOAD IMAGES.\"  width=\"400\" height=\"400\" >\n",
    "\n",
    "\n",
    "From the plot max value at D(x)=1 since D(x) lies between 0 and 1\n",
    "<img src=\"2.JPG\" alt=\"Loss formula Diagram.  PLEASE GIVE PROPER PATH TO LOAD IMAGES.\"  width=\"400\" height=\"400\">\n",
    "\n",
    "\n",
    "From the plot max value at D(G(z))=0 since D(G(z)) lies between 0 and 1\n",
    "\n",
    "\n",
    "Max(log(D(x)) + log(1-D(G(z)))   -----------(4)\n",
    "The objective of the generator is to fool the discriminator. To achieve this D(G(z)=1.\n",
    "Therefore we have to minimize eqn(3) and eqn(2) has no role to play since it corresponds to real data.\n",
    "Min(log(1-D(G(z)))  -------------(5)\n",
    "<img src=\"2.JPG\" alt=\"Loss formula Diagram.  PLEASE GIVE PROPER PATH TO LOAD IMAGES.\"  width=\"400\" height=\"400\">\n",
    "\n",
    "By combining eqn(4) and eqn(5) we can write \n",
    "<img src=\"min_max.JPG\" alt=\"Loss formula Diagram.  PLEASE GIVE PROPER PATH TO LOAD IMAGES.\"  width=\"800\" height=\"800\">\n",
    "\n",
    "<img src=\"forward_backward.JPG\" alt=\"Loss formula Diagram.  PLEASE GIVE PROPER PATH TO LOAD IMAGES.\"  width=\"800\" height=\"800\">\n",
    "During Forward propagation, noise image is given to generator which produces super resoluted images G(s). This is then given to discriminator along with the real image, which gives the probability of 0 to 1.\n",
    "\n",
    "During back propagation, after finding the probabilities, we differentiate the cost function and it updates the weights and biases of generator and discriminiator.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BGG_LOSS(object):\n",
    "\n",
    "    def __init__(self, image_shape):\n",
    "        \n",
    "        self.image_shape = image_shape\n",
    "\n",
    "    # computes content loss\n",
    "    def bgg_loss(self, y_true, y_pred):\n",
    "    \n",
    "        vgg19 = VGG19(include_top=False, weights='imagenet', input_shape=self.image_shape)\n",
    "        vgg19.trainable = False\n",
    "        # Make trainable as False\n",
    "        for l in vgg19.layers:\n",
    "            l.trainable = False\n",
    "        model = Model(inputs=vgg19.input, outputs=vgg19.get_layer('block5_conv4').output)\n",
    "        model.trainable = False\n",
    "    \n",
    "        return K.mean(K.square(model(y_true) - model(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gan_network(discriminator, shape, generator, optimizer):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=shape)\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = Model(inputs=gan_input, outputs=[x,gan_output])\n",
    "    gan.compile(loss=[BGG_LOSS(image_shape).bgg_loss, \"binary_crossentropy\"],\n",
    "                loss_weights=[1., 1e-3],\n",
    "                optimizer=optimizer)\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Training</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SRGAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "image_shape = (512,512,3)# for high resolution image\n",
    "    \n",
    "def train(files,files1, epochs=100, batch_size=8):\n",
    "    x_train_hr,x_test_hr = preprocess(files)\n",
    "    x_train_lr,x_test_lr = preprocess1(files1)\n",
    "    print(\"data processed\")\n",
    "\n",
    "    d_loss_real = 0\n",
    "    d_loss_fake = 0\n",
    "    loss_gan = 0\n",
    "\n",
    "    batch_count = int(x_train_hr.shape[0] / batch_size)\n",
    "    shape = (256, 256, 3) # for low resolution image\n",
    "\n",
    "    generator = Generator(shape).generator()\n",
    "    discriminator = Discriminator(image_shape).discriminator()\n",
    "\n",
    "    adam = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    generator.compile(loss=BGG_LOSS(image_shape).bgg_loss, optimizer=adam)\n",
    "    discriminator.compile(loss=\"binary_crossentropy\", optimizer=adam)\n",
    "\n",
    "    shape = (256, 256, 3)\n",
    "    gan = get_gan_network(discriminator, shape, generator, adam)\n",
    "    loss_file_name = 'loss_'+time.ctime().replace(' ','_').replace(':','')[11:-5]+'.txt'\n",
    "    loss_file = open(loss_file_name,'a')\n",
    "    for e in range(1, epochs+1):\n",
    "        print ('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "        for _ in range(batch_count):\n",
    "            rand_nums = np.random.randint(0, x_train_hr.shape[0], size=batch_size)\n",
    "            image_batch_hr = x_train_hr[rand_nums]\n",
    "            image_batch_lr = x_train_lr[rand_nums]\n",
    "            generated_images_sr = generator.predict(image_batch_lr)\n",
    "            print('Batch-%d is fed to generator.'%(_+1))\n",
    "            real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2\n",
    "            fake_data_Y = np.random.random_sample(batch_size)*0.2\n",
    "\n",
    "            discriminator.trainable = True\n",
    "            print(\"Calculating Losses....\")\n",
    "            d_loss_real = discriminator.train_on_batch(image_batch_hr, real_data_Y)\n",
    "            print(\"Training....d_loss_real\")\n",
    "            d_loss_fake = discriminator.train_on_batch(generated_images_sr, fake_data_Y)\n",
    "            print(\"Training....d_loss_fake\")\n",
    "            d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "\n",
    "            rand_nums = np.random.randint(0, x_train_hr.shape[0], size=batch_size)\n",
    "            image_batch_hr = x_train_hr[rand_nums]\n",
    "            image_batch_lr = x_train_lr[rand_nums]\n",
    "    \n",
    "            gan_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2\n",
    "            discriminator.trainable = False\n",
    "            loss_gan = gan.train_on_batch(image_batch_lr, [image_batch_hr,gan_Y])\n",
    "            print(\"Training....loss_gan\")\n",
    "\n",
    "        print(\"Loss HR , Loss LR, Loss GAN , Dis Loss\")\n",
    "        print(d_loss_real, d_loss_fake, loss_gan , d_loss)\n",
    "        loss_str = ' '.join(list(map(str,[d_loss_real, d_loss_fake, loss_gan , d_loss])))\n",
    "        loss_file.write(loss_str+'\\n')\n",
    "        \n",
    "        if e % (epochs//1) == 0:\n",
    "            plot_generated_images('./output/', e, generator, x_test_hr, x_test_lr)\n",
    "        if e % epochs == 0:\n",
    "            generator.save('./output/gen_model%d.h5' % e)\n",
    "            discriminator.save('./output/dis_model%d.h5' % e)\n",
    "            gan.save('./output/gan_model%d.h5' % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking Training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this code for training your model\n",
    "files = load_data_from_dirs(\"./data1/hr512jpg\", \".jpg\", no_of_images=100) #loading hr images\n",
    "files1 = load_data_from_dirs(\"./data1/lr256jpg\", \".jpg\", no_of_images=100)#loading lr images\n",
    "print(\"data loaded\")\n",
    "train(files,files1,100,2)# train(hr,lr,epoch,batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results After Training:\n",
    "<pre>\n",
    " * Generator Loss vs Epoch: -\n",
    "<img src=\"Generator Loss 2.png\" alt=\"Image not loaded. Please check the path.\">\n",
    " * Image obtained after 1000 epoch:- \n",
    "<img src=\"1000epoochgenerated_image_900.png\" alt=\"Image not loaded. Please check the path.\"> \n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Testing</center>\n",
    "Low Resolution image has the size 256*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this code to test your model\n",
    "image_shape = (256,256,3)\n",
    "\n",
    "def test_model(input_hig_res,input_low_res, model, number_of_images, output_dir):\n",
    "    \n",
    "    x_test_hr = load_test_data_for_model(input_hig_res, 'jpg', number_of_images) \n",
    "    x_test_lr = load_test_data_for_model1(input_low_res, 'jpg', number_of_images)\n",
    "    plot_test_generated_images_for_model(output_dir, model, x_test_hr, x_test_lr)\n",
    "\n",
    "def test_model_for_lr_images(input_low_res, model, number_of_images, output_dir):\n",
    "\n",
    "    x_test_lr = load_test_data(input_low_res, 'jpg', number_of_images)\n",
    "    plot_test_generated_images(output_dir, model, x_test_lr)\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    #give the path accordingly\n",
    "    model_dir = './1000epooch/gen_model900.h5'\n",
    "    test_type = 'test_model'\n",
    "    input_hig_res = './high_image/'# Please specify the path\n",
    "    number_of_images = 1 #How many number of images to test the model\n",
    "    output_dir = './Output/' #Please specify the path\n",
    "    input_low_res = './Low_image/' Please specify the path\n",
    "    loss = BGG_LOSS(image_shape)\n",
    "    model = load_model(model_dir , custom_objects={'bgg_loss': loss.bgg_loss})\n",
    "    \n",
    "    if test_type == 'test_model':\n",
    "        test_model(input_hig_res,input_low_res, model, number_of_images, output_dir)\n",
    "        \n",
    "    elif test_type == 'test_lr_images':\n",
    "        test_model_for_lr_images(input_low_res, model, number_of_images, output_dir)\n",
    "        \n",
    "    else:\n",
    "        print(\"No such option\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Performance Metrics</center>\n",
    "<pre>\n",
    "              <b><h3> * PSNR</b></h3>\n",
    "              <b><h3> * SSIM</b></h3>\n",
    "</pre>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MATLAB code to compute PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TargetImage=imread(‘Target.jpg’);\n",
    "SuperResolutedImage=imread(‘super resoluted.jpg’);\n",
    "n=size(TargetImage);\n",
    "M=n(1);\n",
    "N=n(2);\n",
    "MSE = sum(sum((TargetImage-SuperResolutedImage).^2))/(M*N);\n",
    "PSNR = 10*log10(256*256/MSE);\n",
    "fprintf('\\nMSE: %7.2f ', MSE);\n",
    "fprintf('\\nPSNR: %9.7f dB', PSNR);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSIM:\n",
    "<b>The Structural Similarity Index (SSIM) is a perceptual metric that quantifies image quality degradation* caused by processing such as data compression or by losses in data transmission.</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def mse(imageA, imageB):\n",
    "    # the 'Mean Squared Error' between the two images is the\n",
    "    # sum of the squared difference between the two images;\n",
    "    # NOTE: the two images must have the same dimension\n",
    "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    \n",
    "    # return the MSE, the lower the error, the more \"similar\"\n",
    "    # the two images are\n",
    "    return err\n",
    "\n",
    "def compare_images(imageA, imageB, title):\n",
    "    # compute the mean squared error and structural similarity\n",
    "    # index for the images\n",
    "    m = mse(imageA, imageB)\n",
    "    s = measure.compare_ssim(imageA, imageB)\n",
    "\n",
    "    # setup the figure\n",
    "    fig = plt.figure(title)\n",
    "    plt.suptitle(\"MSE: %.2f, SSIM: %.2f\" % (m, s))\n",
    "    \n",
    "    # show first image\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(image…\n",
    "           \n",
    "super_org = cv2.imread('give path here')\n",
    "original = cv2.imread('give path here')\n",
    "# convert the images to grayscale\n",
    "original = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)\n",
    "super_org = cv2.cvtColor(super_org, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# initialize the figure\n",
    "fig = plt.figure(\"Images\")\n",
    "images = (\"Original\", original), (\"Super\", super_org)\n",
    " \n",
    "# loop over the images\n",
    "for (i, (name, image)) in enumerate(images):\n",
    "    # show the image\n",
    "    ax = fig.add_subplot(1, 3, i + 1)\n",
    "    ax.set_title(name)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    " \n",
    "# show the figure\n",
    "plt.show()\n",
    " \n",
    "# compare the images\n",
    "compare_images(original, original, \"Original vs. Original\")\n",
    "compare_images(original, super_org, \"Original vs. Super\")               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ssmi.jpeg\" alt=\"Image not loaded\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Feature Maps</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./1000epooch/gen_model900.h5' , custom_objects={'bgg_loss': loss.bgg_loss})\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[1].output)\n",
    "# load the imImageage with the required shapelr_trail/bird.jpg\n",
    "img = load_img('./lr_trail/bird.jpg', target_size=(256, 256))\n",
    "# convert the image to an array\n",
    "img = img_to_array(img)\n",
    "# expand dimensions so that it represents a single 'sample'\n",
    "img = expand_dims(img, axis=0)\n",
    "# prepare the image (e.g. scale pixel values for the vgg)\n",
    "img = preprocess_input(img)\n",
    "# get feature map for first hidden layer\n",
    "feature_maps = model.predict(img)\n",
    "# plot all 64 maps in an 8x8 squares\n",
    "square = 8\n",
    "ix = 1\n",
    "pyplot.figure(figsize=(15,15))\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale        \n",
    "        plt.imshow(feature_maps[0, :, :, ix-1])\n",
    "        ix += 1\n",
    "    # show the figure\n",
    "plt.show()\n",
    "print(i)\n",
    "plt.imshow(feature_maps[0, :, :, 1])\n",
    "plt.savefig('./output/layer1' .jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Input</center>\n",
    "<img src=\"bird.jpg\" alt=\"Image not loaded\">\n",
    "\n",
    "## <center>Layer 1</center>\n",
    "\n",
    "<img src=\"layer1.jpg\" alt=\"Image not loaded\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Box Plot</center>\n",
    "<img src=\"psnr.png\" alt=\"Image not loaded\">\n",
    "<img src=\"psnr22.png\" alt=\"Image not loaded\" align='left' width='50%' height='50%'>\n",
    "<img src=\"ssmi1.png\" alt=\"Image not loaded\" align='right' width='50%' height='50%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "- Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network by Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi Twitter\n",
    "- https://github.com/deepak112/Keras-SRGAN\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
